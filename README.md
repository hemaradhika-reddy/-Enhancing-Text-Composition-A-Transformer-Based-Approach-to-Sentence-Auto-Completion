Project Overview
This project focuses on enhancing text composition by developing a transformer-based model for sentence auto-completion. Traditional methods often fail to capture nuanced context and semantics, making them less effective for natural language generation. By leveraging transformer architectures, such as GPT, we aim to predict and generate coherent sentence continuations, assisting writers in generating text more efficiently and effectively.

Dataset
We utilized the "Tweets Blogs News - Swiftkey Dataset 4 million" for training our model. This dataset includes a variety of writing styles from different sources, helping to create a robust NLP model.Methodology
Data Preprocessing: Cleaned and tokenized the dataset, handling emojis, special characters, and out-of-vocabulary words.
Model Building: Implemented transformer-based architectures, focusing on capturing long-term dependencies within sentences.
Training and Evaluation: Trained models using n-gram counts, LSTM, and BiLSTM, evaluating performance based on accuracy and loss metrics.
Results
Improved sentence auto-completion accuracy by leveraging transformer models, capturing long-term dependencies better than traditional methods.
Enhanced writing efficiency and creativity by providing relevant and accurate auto-completions.
Conclusion
This project demonstrates the potential of transformer-based models in improving sentence auto-completion tasks, suggesting future work to integrate hybrid architectures for further performance gains.
